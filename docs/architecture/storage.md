# CAS å­˜å‚¨å¼•æ“æ¶æ„è®¾è®¡

**ç‰ˆæœ¬**: v1.0
**æœ€åæ›´æ–°**: 2026-02-04
**è´Ÿè´£äºº**: Claude AI
**å…³è”æ¨¡å—**: server/internal/storage

---

## ğŸ“‹ ç›®å½•

- [1. è®¾è®¡æ¦‚è¿°](#1-è®¾è®¡æ¦‚è¿°)
- [2. å†…å®¹å¯»å€å­˜å‚¨åŸç†](#2-å†…å®¹å¯»å€å­˜å‚¨åŸç†)
- [3. å»é‡æœºåˆ¶](#3-å»é‡æœºåˆ¶)
- [4. å¼•ç”¨è®¡æ•°ç®¡ç†](#4-å¼•ç”¨è®¡æ•°ç®¡ç†)
- [5. å­˜å‚¨å¼•æ“æ¥å£](#5-å­˜å‚¨å¼•æ“æ¥å£)
- [6. ç›®å½•ç»“æ„è®¾è®¡](#6-ç›®å½•ç»“æ„è®¾è®¡)
- [7. åƒåœ¾å›æ”¶ç­–ç•¥](#7-åƒåœ¾å›æ”¶ç­–ç•¥)
- [8. æ€§èƒ½ä¼˜åŒ–](#8-æ€§èƒ½ä¼˜åŒ–)

---

## 1. è®¾è®¡æ¦‚è¿°

### 1.1 ä»€ä¹ˆæ˜¯ CASï¼Ÿ

**CAS (Content-Addressable Storage)** æ˜¯ä¸€ç§é€šè¿‡å†…å®¹æœ¬èº«ï¼ˆè€Œéä½ç½®ï¼‰æ¥å¯»å€çš„å­˜å‚¨æ–¹å¼ã€‚æ–‡ä»¶çš„å”¯ä¸€æ ‡è¯†ç¬¦æ˜¯å…¶å†…å®¹çš„å“ˆå¸Œå€¼ï¼ˆSHA-256ï¼‰ï¼Œè€Œéä¼ ç»Ÿçš„æ–‡ä»¶è·¯å¾„ã€‚

### 1.2 è®¾è®¡ç›®æ ‡

1. **å…¨å±€å»é‡**: ç›¸åŒå†…å®¹çš„æ–‡ä»¶åœ¨ç‰©ç†å±‚ä»…å­˜å‚¨ä¸€ä»½
2. **ç©ºé—´é«˜æ•ˆ**: å¤§å¹…é™ä½å­˜å‚¨æˆæœ¬ï¼Œå°¤å…¶åœ¨å¤§é‡é‡å¤æ–‡ä»¶åœºæ™¯
3. **æ•°æ®å®Œæ•´æ€§**: é€šè¿‡å“ˆå¸ŒéªŒè¯ç¡®ä¿æ–‡ä»¶æœªè¢«ç¯¡æ”¹
4. **å®‰å…¨éš”ç¦»**: ç”¨æˆ·çœ‹åˆ°çš„é€»è¾‘æ–‡ä»¶ç›¸äº’ç‹¬ç«‹ï¼Œç‰©ç†å±‚å…±äº«å­˜å‚¨

### 1.3 æ¶æ„æ€»è§ˆ

```mermaid
graph TD
    U1[ç”¨æˆ· A: report.pdf] --> LF1[é€»è¾‘æ–‡ä»¶è¡¨: files_metadata]
    U2[ç”¨æˆ· B: report.pdf] --> LF2[é€»è¾‘æ–‡ä»¶è¡¨: files_metadata]
    U3[ç”¨æˆ· C: document.pdf] --> LF3[é€»è¾‘æ–‡ä»¶è¡¨: files_metadata]

    LF1 --> |hash: aabbcc...| PF[ç‰©ç†æ–‡ä»¶è¡¨: file_blobs]
    LF2 --> |hash: aabbcc...| PF
    LF3 --> |hash: aabbcc...| PF

    PF --> |ref_count: 3| Storage[ç‰©ç†å­˜å‚¨: /aa/bb/aabbcc...]

    style PF fill:#ff6b6b
    style Storage fill:#4ecdc4
```

**è¯´æ˜**: ä¸‰ä¸ªç”¨æˆ·ä¸Šä¼ äº†ç›¸åŒå†…å®¹çš„ PDF æ–‡ä»¶ï¼Œç‰©ç†å±‚ä»…å­˜å‚¨ä¸€ä»½ï¼Œé€šè¿‡å¼•ç”¨è®¡æ•°ç»´æŠ¤å…³ç³»ã€‚

---

## 2. å†…å®¹å¯»å€å­˜å‚¨åŸç†

### 2.1 å“ˆå¸Œè®¡ç®—

ä½¿ç”¨ **SHA-256** ç®—æ³•è®¡ç®—æ–‡ä»¶å†…å®¹çš„å”¯ä¸€æ ‡è¯†ï¼š

```
æ–‡ä»¶å†…å®¹ â†’ SHA-256 â†’ aabbccddeeff1122334455667788990011223344556677889900aabbccddeeff
```

**ç‰¹æ€§**:
- **å”¯ä¸€æ€§**: ä¸¤ä¸ªä¸åŒå†…å®¹çš„æ–‡ä»¶å‡ ä¹ä¸å¯èƒ½äº§ç”Ÿç›¸åŒå“ˆå¸Œï¼ˆç¢°æ’æ¦‚ç‡ < 2^-256ï¼‰
- **ç¡®å®šæ€§**: ç›¸åŒå†…å®¹æ€»æ˜¯äº§ç”Ÿç›¸åŒå“ˆå¸Œ
- **å•å‘æ€§**: æ— æ³•ä»å“ˆå¸Œåæ¨åŸå§‹å†…å®¹

### 2.2 å‰ç«¯å“ˆå¸Œè®¡ç®—

ä¸ºäº†æ”¯æŒç§’ä¼ ï¼Œå‰ç«¯éœ€è¦åœ¨ä¸Šä¼ å‰è®¡ç®—æ–‡ä»¶å“ˆå¸Œï¼š

```javascript
// web/src/workers/sha256.worker.ts
import CryptoJS from 'crypto-js';

self.onmessage = async (e) => {
  const file = e.data.file;
  const chunkSize = 2 * 1024 * 1024; // 2MB per chunk
  const chunks = Math.ceil(file.size / chunkSize);

  const hasher = CryptoJS.algo.SHA256.create();

  for (let i = 0; i < chunks; i++) {
    const start = i * chunkSize;
    const end = Math.min(start + chunkSize, file.size);
    const chunk = file.slice(start, end);

    const arrayBuffer = await chunk.arrayBuffer();
    const wordArray = CryptoJS.lib.WordArray.create(arrayBuffer);

    hasher.update(wordArray);

    // æŠ¥å‘Šè¿›åº¦
    self.postMessage({
      type: 'progress',
      progress: ((i + 1) / chunks) * 100
    });
  }

  const hash = hasher.finalize().toString();

  self.postMessage({
    type: 'complete',
    hash: hash
  });
};
```

**ä¸ºä»€ä¹ˆåœ¨å‰ç«¯è®¡ç®—ï¼Ÿ**
- âœ… æ”¯æŒç§’ä¼ ï¼ˆä¸Šä¼ å‰æ£€æµ‹ï¼‰
- âœ… å‡å°‘æœåŠ¡å™¨è®¡ç®—è´Ÿæ‹…
- âœ… æå‰éªŒè¯æ–‡ä»¶å®Œæ•´æ€§

### 2.3 åç«¯å“ˆå¸ŒéªŒè¯

åç«¯åœ¨æ¥æ”¶æ–‡ä»¶æ—¶éœ€è¦é‡æ–°è®¡ç®—å“ˆå¸ŒéªŒè¯ï¼š

```go
// server/internal/crypto/hash.go
func CalculateSHA256(reader io.Reader) (string, error) {
    hasher := sha256.New()
    if _, err := io.Copy(hasher, reader); err != nil {
        return "", err
    }
    return hex.EncodeToString(hasher.Sum(nil)), nil
}
```

**åŒé‡éªŒè¯æµç¨‹**:
```
å‰ç«¯è®¡ç®— hash â†’ åç«¯æ¥æ”¶ â†’ åç«¯é‡æ–°è®¡ç®— â†’ æ¯”å¯¹ä¸€è‡´ â†’ å­˜å‚¨
```

---

## 3. å»é‡æœºåˆ¶

### 3.1 å»é‡æµç¨‹

```mermaid
sequenceDiagram
    participant User as ç”¨æˆ·
    participant Frontend as å‰ç«¯
    participant Backend as åç«¯
    participant DB as PostgreSQL
    participant Storage as å­˜å‚¨

    User->>Frontend: ä¸Šä¼  file.pdf (2MB)
    Frontend->>Frontend: Web Worker è®¡ç®— SHA-256

    Frontend->>Backend: POST /api/files/check<br/>{hash: "aabbcc..."}

    Backend->>DB: SELECT * FROM file_blobs<br/>WHERE hash = 'aabbcc...'

    alt æ–‡ä»¶å·²å­˜åœ¨
        DB-->>Backend: è®°å½•å­˜åœ¨ (ref_count = 2)
        Backend-->>Frontend: {exists: true}

        Frontend->>Backend: POST /api/files<br/>{hash, filename, size}

        Backend->>DB: BEGIN TRANSACTION
        Backend->>DB: INSERT INTO files_metadata
        Backend->>DB: UPDATE file_blobs<br/>SET ref_count = ref_count + 1
        Backend->>DB: COMMIT

        Backend-->>Frontend: ç§’ä¼ æˆåŠŸ âœ…
    else æ–‡ä»¶ä¸å­˜åœ¨
        DB-->>Backend: æ— è®°å½•
        Backend-->>Frontend: {exists: false}

        Frontend->>Backend: Tus åè®®ä¸Šä¼ æ–‡ä»¶

        Backend->>Backend: åŠ å¯†æ–‡ä»¶æµ
        Backend->>Storage: å­˜å‚¨ç‰©ç†æ–‡ä»¶
        Backend->>DB: INSERT file_blobs (ref_count = 1)
        Backend->>DB: INSERT files_metadata

        Backend-->>Frontend: ä¸Šä¼ å®Œæˆ âœ…
    end
```

### 3.2 ç§’ä¼ ä¼˜åŠ¿

**åœºæ™¯**: å…¬å¸å†… 100 äººéƒ½éœ€è¦ä¸‹è½½åŒä¸€ä»½ 500MB çš„å®‰è£…åŒ…

**ä¼ ç»Ÿæ–¹æ¡ˆ**:
- 100 äººå„ä¸Šä¼ ä¸€æ¬¡ â†’ æ€»ä¸Šä¼ æµé‡ = 50GB
- å­˜å‚¨ç©ºé—´å ç”¨ = 50GB

**CAS æ–¹æ¡ˆ**:
- ç¬¬ 1 äººä¸Šä¼  500MBï¼Œåç»­ 99 äººç§’ä¼ ï¼ˆ0 æµé‡ï¼‰
- å­˜å‚¨ç©ºé—´å ç”¨ = 500MB
- **èŠ‚çœ**: 99% æµé‡ + 99% å­˜å‚¨

---

## 4. å¼•ç”¨è®¡æ•°ç®¡ç†

### 4.1 ä¸ºä»€ä¹ˆéœ€è¦å¼•ç”¨è®¡æ•°ï¼Ÿ

```
ç‰©ç†æ–‡ä»¶: /storage/aa/bb/aabbcc...
    â†‘
    |--- ç”¨æˆ· A çš„ report.pdf (é€»è¾‘æ–‡ä»¶)
    |--- ç”¨æˆ· B çš„ backup.pdf (é€»è¾‘æ–‡ä»¶)
    |--- ç”¨æˆ· C çš„ doc.pdf     (é€»è¾‘æ–‡ä»¶)

å¼•ç”¨è®¡æ•° = 3
```

**é—®é¢˜**: å½“ç”¨æˆ· A åˆ é™¤æ–‡ä»¶æ—¶ï¼Œæ˜¯å¦åº”è¯¥åˆ é™¤ç‰©ç†æ–‡ä»¶ï¼Ÿ

**ç­”æ¡ˆ**: ä¸åº”è¯¥ï¼å› ä¸ºç”¨æˆ· B å’Œ C è¿˜åœ¨å¼•ç”¨ã€‚åªæœ‰å½“ `ref_count = 0` æ—¶æ‰èƒ½åˆ é™¤ã€‚

### 4.2 æ•°æ®åº“è¡¨ç»“æ„

#### file_blobs (ç‰©ç†æ–‡ä»¶è¡¨)

```sql
CREATE TABLE file_blobs (
    hash VARCHAR(64) PRIMARY KEY,              -- SHA-256 å“ˆå¸Œå€¼
    store_path VARCHAR(255) NOT NULL,          -- ç‰©ç†å­˜å‚¨è·¯å¾„
    encrypted_dek TEXT NOT NULL,               -- åŠ å¯†åçš„ DEK
    size BIGINT NOT NULL,                      -- æ–‡ä»¶å¤§å° (bytes)
    mime_type VARCHAR(128),                    -- MIME ç±»å‹
    ref_count INT NOT NULL DEFAULT 1,          -- å¼•ç”¨è®¡æ•° â­
    is_banned BOOLEAN DEFAULT FALSE,           -- æ˜¯å¦è¢«ç®¡ç†å‘˜ç¦æ­¢
    created_at TIMESTAMP DEFAULT NOW(),
    updated_at TIMESTAMP DEFAULT NOW()
);

CREATE INDEX idx_ref_count ON file_blobs(ref_count);
```

#### files_metadata (é€»è¾‘æ–‡ä»¶è¡¨)

```sql
CREATE TABLE files_metadata (
    id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    user_id UUID NOT NULL REFERENCES users(id),
    file_blob_hash VARCHAR(64) REFERENCES file_blobs(hash) ON DELETE RESTRICT,
    filename VARCHAR(255) NOT NULL,            -- ç”¨æˆ·è‡ªå®šä¹‰æ–‡ä»¶å
    size BIGINT NOT NULL,
    created_at TIMESTAMP DEFAULT NOW(),
    expires_at TIMESTAMP,                      -- æ–‡ä»¶è¿‡æœŸæ—¶é—´
    deleted_at TIMESTAMP,                      -- è½¯åˆ é™¤æ—¶é—´ â­

    FOREIGN KEY (file_blob_hash) REFERENCES file_blobs(hash)
);

CREATE INDEX idx_user_files ON files_metadata(user_id, deleted_at);
CREATE INDEX idx_blob_hash ON files_metadata(file_blob_hash);
```

### 4.3 å¼•ç”¨è®¡æ•°æ“ä½œï¼ˆå…³é”®ï¼ï¼‰

#### 4.3.1 ç”¨æˆ·ä¸Šä¼ æ–‡ä»¶

```sql
-- æ–¹æ¡ˆ A: æ–‡ä»¶å·²å­˜åœ¨ï¼ˆç§’ä¼ ï¼‰
BEGIN;
    INSERT INTO files_metadata (user_id, file_blob_hash, filename, size)
    VALUES ('user-uuid', 'aabbcc...', 'my_file.pdf', 2048576);

    UPDATE file_blobs
    SET ref_count = ref_count + 1,
        updated_at = NOW()
    WHERE hash = 'aabbcc...';
COMMIT;

-- æ–¹æ¡ˆ B: æ–‡ä»¶ä¸å­˜åœ¨ï¼ˆé¦–æ¬¡ä¸Šä¼ ï¼‰
BEGIN;
    INSERT INTO file_blobs (hash, store_path, encrypted_dek, size, ref_count)
    VALUES ('aabbcc...', '/aa/bb/aabbcc...', 'encrypted_dek_data', 2048576, 1);

    INSERT INTO files_metadata (user_id, file_blob_hash, filename, size)
    VALUES ('user-uuid', 'aabbcc...', 'my_file.pdf', 2048576);
COMMIT;
```

#### 4.3.2 ç”¨æˆ·åˆ é™¤æ–‡ä»¶ï¼ˆè½¯åˆ é™¤ï¼‰

```sql
BEGIN;
    -- æ ‡è®°é€»è¾‘æ–‡ä»¶ä¸ºå·²åˆ é™¤
    UPDATE files_metadata
    SET deleted_at = NOW()
    WHERE id = 'file-uuid' AND user_id = 'user-uuid';

    -- å‡å°‘å¼•ç”¨è®¡æ•°
    UPDATE file_blobs
    SET ref_count = ref_count - 1,
        updated_at = NOW()
    WHERE hash = (
        SELECT file_blob_hash FROM files_metadata WHERE id = 'file-uuid'
    );
COMMIT;
```

#### 4.3.3 åå° GC ç¡¬åˆ é™¤

```sql
-- 1. åˆ é™¤è½¯åˆ é™¤è¶…è¿‡ 7 å¤©çš„å…ƒæ•°æ®
BEGIN;
    DELETE FROM files_metadata
    WHERE deleted_at IS NOT NULL
      AND deleted_at < NOW() - INTERVAL '7 days';
COMMIT;

-- 2. æ¸…ç†å¼•ç”¨è®¡æ•°ä¸º 0 çš„ç‰©ç†æ–‡ä»¶
BEGIN;
    -- è·å–æ‰€æœ‰ ref_count = 0 çš„æ–‡ä»¶
    SELECT hash, store_path
    FROM file_blobs
    WHERE ref_count = 0;

    -- ä»ç‰©ç†å­˜å‚¨åˆ é™¤æ–‡ä»¶
    -- (åœ¨åº”ç”¨å±‚è°ƒç”¨ storage.Delete(hash))

    -- ä»æ•°æ®åº“åˆ é™¤è®°å½•
    DELETE FROM file_blobs WHERE ref_count = 0;
COMMIT;
```

### 4.4 æ•°æ®ä¸€è‡´æ€§ä¿è¯

**å…³é”®åŸåˆ™**: å¼•ç”¨è®¡æ•°çš„å¢å‡**å¿…é¡»åœ¨æ•°æ®åº“äº‹åŠ¡ä¸­å®Œæˆ**ï¼Œä¸¥ç¦åº”ç”¨å±‚è®¡ç®—ã€‚

#### âŒ é”™è¯¯ç¤ºä¾‹ï¼ˆç«æ€æ¡ä»¶ï¼‰

```go
// é”™è¯¯ï¼šåº”ç”¨å±‚è®¡ç®— ref_count
currentCount := getRefCount(hash)
newCount := currentCount + 1
updateRefCount(hash, newCount)  // ç«æ€æ¡ä»¶ï¼ä¸¤ä¸ªå¹¶å‘è¯·æ±‚å¯èƒ½å¯¼è‡´è®¡æ•°é”™è¯¯
```

#### âœ… æ­£ç¡®ç¤ºä¾‹ï¼ˆæ•°æ®åº“åŸå­æ“ä½œï¼‰

```go
// æ­£ç¡®ï¼šä½¿ç”¨æ•°æ®åº“åŸå­æ“ä½œ
tx := db.Begin()
defer tx.Rollback()

// åˆ›å»ºé€»è¾‘æ–‡ä»¶
if err := tx.Create(&metadata).Error; err != nil {
    return err
}

// åŸå­åœ°å¢åŠ å¼•ç”¨è®¡æ•°
if err := tx.Exec("UPDATE file_blobs SET ref_count = ref_count + 1 WHERE hash = ?", hash).Error; err != nil {
    return err
}

tx.Commit()
```

### 4.5 å¼•ç”¨è®¡æ•°ä¸€è‡´æ€§æ£€æŸ¥

å®šæœŸè¿è¡Œä¸€è‡´æ€§æ£€æŸ¥è„šæœ¬ï¼Œç¡®ä¿ `ref_count` å‡†ç¡®ï¼š

```sql
-- æ£€æŸ¥ä¸ä¸€è‡´çš„è®°å½•
SELECT
    b.hash,
    b.ref_count AS stored_count,
    COUNT(m.id) AS actual_count
FROM file_blobs b
LEFT JOIN files_metadata m ON m.file_blob_hash = b.hash AND m.deleted_at IS NULL
GROUP BY b.hash
HAVING b.ref_count != COUNT(m.id);

-- ä¿®å¤å¼•ç”¨è®¡æ•°
UPDATE file_blobs b
SET ref_count = (
    SELECT COUNT(*)
    FROM files_metadata m
    WHERE m.file_blob_hash = b.hash AND m.deleted_at IS NULL
)
WHERE hash IN (
    SELECT hash FROM file_blobs WHERE ref_count != (
        SELECT COUNT(*) FROM files_metadata WHERE file_blob_hash = hash AND deleted_at IS NULL
    )
);
```

---

## 5. å­˜å‚¨å¼•æ“æ¥å£

### 5.1 æ¥å£è®¾è®¡

```go
// server/internal/storage/interface.go
package storage

import "io"

// Engine å®šä¹‰å­˜å‚¨å¼•æ“æ¥å£
type Engine interface {
    // Put å­˜å‚¨æ–‡ä»¶ï¼Œè¿”å›å­˜å‚¨è·¯å¾„
    Put(hash string, reader io.Reader) error

    // Get è¯»å–æ–‡ä»¶ï¼Œè¿”å›å¯è¯»æµ
    Get(hash string) (io.ReadCloser, error)

    // Delete åˆ é™¤æ–‡ä»¶
    Delete(hash string) error

    // Exists æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
    Exists(hash string) (bool, error)

    // Stat è·å–æ–‡ä»¶ä¿¡æ¯
    Stat(hash string) (*FileInfo, error)
}

type FileInfo struct {
    Hash      string
    Size      int64
    StorePath string
}
```

### 5.2 Local å­˜å‚¨å®ç°

```go
// server/internal/storage/local.go
package storage

import (
    "io"
    "os"
    "path/filepath"
)

type LocalEngine struct {
    basePath string // å¦‚ /data/storage
}

func NewLocalEngine(basePath string) *LocalEngine {
    return &LocalEngine{basePath: basePath}
}

func (e *LocalEngine) Put(hash string, reader io.Reader) error {
    // ç”Ÿæˆè·¯å¾„: /data/storage/aa/bb/aabbcc...
    path := e.generatePath(hash)

    // ç¡®ä¿ç›®å½•å­˜åœ¨
    if err := os.MkdirAll(filepath.Dir(path), 0755); err != nil {
        return err
    }

    // åˆ›å»ºæ–‡ä»¶
    file, err := os.Create(path)
    if err != nil {
        return err
    }
    defer file.Close()

    // å†™å…¥æ•°æ®
    _, err = io.Copy(file, reader)
    return err
}

func (e *LocalEngine) Get(hash string) (io.ReadCloser, error) {
    path := e.generatePath(hash)
    return os.Open(path)
}

func (e *LocalEngine) Delete(hash string) error {
    path := e.generatePath(hash)
    return os.Remove(path)
}

func (e *LocalEngine) Exists(hash string) (bool, error) {
    path := e.generatePath(hash)
    _, err := os.Stat(path)
    if os.IsNotExist(err) {
        return false, nil
    }
    return err == nil, err
}

func (e *LocalEngine) generatePath(hash string) string {
    // hash = aabbccddeeff...
    // path = /data/storage/aa/bb/aabbccddeeff...
    return filepath.Join(e.basePath, hash[:2], hash[2:4], hash)
}
```

### 5.3 S3 å­˜å‚¨å®ç°

```go
// server/internal/storage/s3.go
package storage

import (
    "io"
    "github.com/aws/aws-sdk-go/aws"
    "github.com/aws/aws-sdk-go/service/s3"
)

type S3Engine struct {
    client *s3.S3
    bucket string
}

func NewS3Engine(endpoint, region, accessKey, secretKey, bucket string) *S3Engine {
    // åˆå§‹åŒ– S3 å®¢æˆ·ç«¯
    sess := session.Must(session.NewSession(&aws.Config{
        Endpoint:         aws.String(endpoint),
        Region:           aws.String(region),
        Credentials:      credentials.NewStaticCredentials(accessKey, secretKey, ""),
        S3ForcePathStyle: aws.Bool(true),
    }))

    return &S3Engine{
        client: s3.New(sess),
        bucket: bucket,
    }
}

func (e *S3Engine) Put(hash string, reader io.Reader) error {
    key := e.generateKey(hash)

    _, err := e.client.PutObject(&s3.PutObjectInput{
        Bucket: aws.String(e.bucket),
        Key:    aws.String(key),
        Body:   reader,
    })
    return err
}

func (e *S3Engine) Get(hash string) (io.ReadCloser, error) {
    key := e.generateKey(hash)

    result, err := e.client.GetObject(&s3.GetObjectInput{
        Bucket: aws.String(e.bucket),
        Key:    aws.String(key),
    })
    if err != nil {
        return nil, err
    }

    return result.Body, nil
}

func (e *S3Engine) Delete(hash string) error {
    key := e.generateKey(hash)

    _, err := e.client.DeleteObject(&s3.DeleteObjectInput{
        Bucket: aws.String(e.bucket),
        Key:    aws.String(key),
    })
    return err
}

func (e *S3Engine) Exists(hash string) (bool, error) {
    key := e.generateKey(hash)

    _, err := e.client.HeadObject(&s3.HeadObjectInput{
        Bucket: aws.String(e.bucket),
        Key:    aws.String(key),
    })

    if err != nil {
        if aerr, ok := err.(awserr.Error); ok && aerr.Code() == s3.ErrCodeNoSuchKey {
            return false, nil
        }
        return false, err
    }

    return true, nil
}

func (e *S3Engine) generateKey(hash string) string {
    // key = aa/bb/aabbccddeeff...
    return fmt.Sprintf("%s/%s/%s", hash[:2], hash[2:4], hash)
}
```

### 5.4 é…ç½®ä¸åˆå§‹åŒ–

```go
// server/internal/config/storage.go
func InitStorageEngine(cfg *Config) storage.Engine {
    switch cfg.StorageType {
    case "local":
        return storage.NewLocalEngine(cfg.StoragePath)

    case "s3":
        return storage.NewS3Engine(
            cfg.S3Endpoint,
            cfg.S3Region,
            cfg.S3AccessKey,
            cfg.S3SecretKey,
            cfg.S3Bucket,
        )

    default:
        log.Fatalf("Unknown storage type: %s", cfg.StorageType)
        return nil
    }
}
```

---

## 6. ç›®å½•ç»“æ„è®¾è®¡

### 6.1 ä¸ºä»€ä¹ˆä½¿ç”¨ 2 çº§å“ˆå¸Œåˆ†ç‰‡ï¼Ÿ

**é—®é¢˜**: å¦‚æœå°†æ‰€æœ‰æ–‡ä»¶å­˜å‚¨åœ¨å•ä¸€ç›®å½•ï¼š

```
/data/storage/
â”œâ”€â”€ aabbccddeeff11223344...
â”œâ”€â”€ 112233445566778899aa...
â”œâ”€â”€ 223344556677889900bb...
â””â”€â”€ ... (æ•°ç™¾ä¸‡ä¸ªæ–‡ä»¶)
```

**æ€§èƒ½é—®é¢˜**:
- âŒ å•ç›®å½•æ–‡ä»¶æ•°è¿‡å¤šï¼Œæ–‡ä»¶ç³»ç»Ÿæ€§èƒ½ä¸‹é™ï¼ˆext4 å•ç›®å½•æœ€ä¼˜ < 10,000 æ–‡ä»¶ï¼‰
- âŒ `ls` å‘½ä»¤æ‰§è¡Œç¼“æ…¢
- âŒ æ–‡ä»¶æŸ¥æ‰¾æ•ˆç‡ä½

**è§£å†³æ–¹æ¡ˆ**: ä½¿ç”¨å“ˆå¸Œå‰ç¼€è¿›è¡Œåˆ†ç‰‡

```
/data/storage/
â”œâ”€â”€ aa/
â”‚   â”œâ”€â”€ bb/
â”‚   â”‚   â”œâ”€â”€ aabbccddeeff11223344...
â”‚   â”‚   â””â”€â”€ aabb11223344556677...
â”‚   â””â”€â”€ cc/
â”‚       â””â”€â”€ aacc99887766554433...
â”œâ”€â”€ ab/
â”‚   â””â”€â”€ cd/
â”‚       â””â”€â”€ abcd...
â””â”€â”€ ...
```

### 6.2 åˆ†ç‰‡ç­–ç•¥

```
hash = aabbccddeeff1122334455667788990011223344556677889900aabbccddeeff
        â†“    â†“
       aa   bb

ç›®å½•ç»“æ„ = /data/storage/aa/bb/aabbccddeeff...
```

**åˆ†ç‰‡æ•ˆæœ**:
- ç¬¬ 1 çº§ç›®å½•: 256 ä¸ª (00-FF)
- ç¬¬ 2 çº§ç›®å½•: 256 ä¸ª (00-FF)
- **æ€»åˆ†ç‰‡æ•°**: 256 Ã— 256 = 65,536 ä¸ªç›®å½•

**ä¼˜åŠ¿**:
- âœ… å³ä½¿æœ‰ 1000 ä¸‡ä¸ªæ–‡ä»¶ï¼Œæ¯ä¸ªç›®å½•å¹³å‡ä»… ~150 ä¸ªæ–‡ä»¶
- âœ… æ–‡ä»¶ç³»ç»Ÿæ€§èƒ½ä¿æŒæœ€ä¼˜
- âœ… æ”¯æŒæ°´å¹³æ‰©å±•ï¼ˆæŒ‰ç›®å½•åˆ†ç‰‡åˆ°ä¸åŒç£ç›˜ï¼‰

### 6.3 è·¯å¾„ç”Ÿæˆç¤ºä¾‹

```go
func generateStoragePath(hash string) string {
    // hash = "a1b2c3d4e5f6..."
    // return "/data/storage/a1/b2/a1b2c3d4e5f6..."

    if len(hash) < 4 {
        return ""
    }

    return filepath.Join(
        "/data/storage",
        hash[:2],    // ç¬¬ 1 çº§: a1
        hash[2:4],   // ç¬¬ 2 çº§: b2
        hash,        // å®Œæ•´å“ˆå¸Œä½œä¸ºæ–‡ä»¶å
    )
}
```

---

## 7. åƒåœ¾å›æ”¶ç­–ç•¥

### 7.1 ä¸‰é˜¶æ®µ GC è®¾è®¡

```mermaid
graph LR
    A[ç”¨æˆ·åˆ é™¤æ–‡ä»¶] --> B[æ ‡è®° deleted_at]
    B --> C{ç­‰å¾… 7 å¤©}
    C --> D[GC-1: ç¡¬åˆ é™¤å…ƒæ•°æ®]
    D --> E[ref_count - 1]
    E --> F{ref_count = 0?}
    F -->|æ˜¯| G[GC-2: åˆ é™¤ç‰©ç†æ–‡ä»¶]
    F -->|å¦| H[ä¿ç•™ç‰©ç†æ–‡ä»¶]

    style B fill:#ffe66d
    style D fill:#ff6b6b
    style G fill:#ff6b6b
```

### 7.2 GC ä»»åŠ¡å®ç°

#### GC-1: å…ƒæ•°æ®æ¸…ç†ï¼ˆæ¯æ—¥å‡Œæ™¨ 2:00ï¼‰

```go
// server/internal/tasks/gc.go
func CleanupSoftDeletedMetadata() error {
    log.Info("Starting GC: Cleanup soft-deleted metadata")

    tx := db.Begin()
    defer tx.Rollback()

    // æŸ¥è¯¢è½¯åˆ é™¤è¶…è¿‡ 7 å¤©çš„è®°å½•
    var toDelete []FilesMetadata
    if err := tx.Where("deleted_at < ?", time.Now().Add(-7*24*time.Hour)).
        Find(&toDelete).Error; err != nil {
        return err
    }

    // æ‰¹é‡å‡å°‘å¼•ç”¨è®¡æ•°
    for _, metadata := range toDelete {
        if err := tx.Exec("UPDATE file_blobs SET ref_count = ref_count - 1 WHERE hash = ?",
            metadata.FileBlobHash).Error; err != nil {
            return err
        }
    }

    // åˆ é™¤å…ƒæ•°æ®è®°å½•
    if err := tx.Where("deleted_at < ?", time.Now().Add(-7*24*time.Hour)).
        Delete(&FilesMetadata{}).Error; err != nil {
        return err
    }

    tx.Commit()

    log.Infof("GC: Cleaned up %d metadata records", len(toDelete))
    return nil
}
```

#### GC-2: å­¤å„¿æ–‡ä»¶æ¸…ç†ï¼ˆæ¯æ—¥å‡Œæ™¨ 3:00ï¼‰

```go
func CleanupOrphanFiles(storage storage.Engine) error {
    log.Info("Starting GC: Cleanup orphan files")

    tx := db.Begin()
    defer tx.Rollback()

    // æŸ¥è¯¢å¼•ç”¨è®¡æ•°ä¸º 0 çš„æ–‡ä»¶
    var orphans []FileBlob
    if err := tx.Where("ref_count = 0").Find(&orphans).Error; err != nil {
        return err
    }

    deletedCount := 0
    for _, blob := range orphans {
        // ä»ç‰©ç†å­˜å‚¨åˆ é™¤
        if err := storage.Delete(blob.Hash); err != nil {
            log.Errorf("Failed to delete file %s: %v", blob.Hash, err)
            continue
        }

        // ä»æ•°æ®åº“åˆ é™¤
        if err := tx.Delete(&blob).Error; err != nil {
            log.Errorf("Failed to delete blob record %s: %v", blob.Hash, err)
            continue
        }

        deletedCount++
    }

    tx.Commit()

    log.Infof("GC: Cleaned up %d orphan files", deletedCount)
    return nil
}
```

#### GC-3: ç¢ç‰‡æ¸…ç†ï¼ˆæ¯å°æ—¶ï¼‰

```go
func CleanupUploadFragments(storage storage.Engine) error {
    log.Info("Starting GC: Cleanup upload fragments")

    // æŸ¥è¯¢ä¸Šä¼ ä¸­æ–­çš„ä¸´æ—¶æ–‡ä»¶ï¼ˆçŠ¶æ€ä¸º uploading ä¸”è¶…è¿‡ 24 å°æ—¶ï¼‰
    var fragments []UploadSession
    if err := db.Where("status = ? AND updated_at < ?", "uploading",
        time.Now().Add(-24*time.Hour)).Find(&fragments).Error; err != nil {
        return err
    }

    deletedCount := 0
    for _, session := range fragments {
        // åˆ é™¤ä¸´æ—¶æ–‡ä»¶
        if err := storage.Delete(session.TempHash); err != nil {
            log.Errorf("Failed to delete fragment %s: %v", session.TempHash, err)
            continue
        }

        // åˆ é™¤ä¼šè¯è®°å½•
        db.Delete(&session)
        deletedCount++
    }

    log.Infof("GC: Cleaned up %d upload fragments", deletedCount)
    return nil
}
```

### 7.3 å®šæ—¶ä»»åŠ¡è°ƒåº¦

```go
// server/cmd/server/main.go
func startGarbageCollector(storage storage.Engine) {
    // GC-1: æ¯æ—¥å‡Œæ™¨ 2:00 æ¸…ç†å…ƒæ•°æ®
    gocron.Every(1).Day().At("02:00").Do(func() {
        if err := tasks.CleanupSoftDeletedMetadata(); err != nil {
            log.Errorf("GC-1 failed: %v", err)
        }
    })

    // GC-2: æ¯æ—¥å‡Œæ™¨ 3:00 æ¸…ç†å­¤å„¿æ–‡ä»¶
    gocron.Every(1).Day().At("03:00").Do(func() {
        if err := tasks.CleanupOrphanFiles(storage); err != nil {
            log.Errorf("GC-2 failed: %v", err)
        }
    })

    // GC-3: æ¯å°æ—¶æ¸…ç†ä¸Šä¼ ç¢ç‰‡
    gocron.Every(1).Hour().Do(func() {
        if err := tasks.CleanupUploadFragments(storage); err != nil {
            log.Errorf("GC-3 failed: %v", err)
        }
    })

    gocron.Start()
}
```

---

## 8. æ€§èƒ½ä¼˜åŒ–

### 8.1 æ•°æ®åº“ç´¢å¼•

```sql
-- åŠ é€Ÿå¼•ç”¨è®¡æ•°æŸ¥è¯¢
CREATE INDEX idx_ref_count ON file_blobs(ref_count);

-- åŠ é€Ÿç”¨æˆ·æ–‡ä»¶æŸ¥è¯¢
CREATE INDEX idx_user_files ON files_metadata(user_id, deleted_at);

-- åŠ é€Ÿå“ˆå¸ŒæŸ¥æ‰¾
CREATE INDEX idx_blob_hash ON files_metadata(file_blob_hash);

-- åŠ é€Ÿ GC æŸ¥è¯¢
CREATE INDEX idx_deleted_at ON files_metadata(deleted_at) WHERE deleted_at IS NOT NULL;
```

### 8.2 å¹¶å‘æ§åˆ¶

#### ä¸Šä¼ é”ï¼ˆRedis åˆ†å¸ƒå¼é”ï¼‰

```go
// é˜²æ­¢å¹¶å‘ä¸Šä¼ åŒä¸€æ–‡ä»¶å¯¼è‡´é‡å¤å­˜å‚¨
func acquireUploadLock(hash string) (bool, error) {
    key := fmt.Sprintf("upload_lock:%s", hash)
    success, err := redis.SetNX(key, "locked", 10*time.Minute).Result()
    return success, err
}

func releaseUploadLock(hash string) error {
    key := fmt.Sprintf("upload_lock:%s", hash)
    return redis.Del(key).Err()
}
```

### 8.3 ç¼“å­˜ç­–ç•¥

#### çƒ­ç‚¹æ–‡ä»¶ç¼“å­˜

```go
// ä½¿ç”¨ Redis ç¼“å­˜é«˜é¢‘è®¿é—®çš„æ–‡ä»¶å…ƒæ•°æ®
func getFileBlobWithCache(hash string) (*FileBlob, error) {
    cacheKey := fmt.Sprintf("blob:%s", hash)

    // å°è¯•ä»ç¼“å­˜è¯»å–
    if cached, err := redis.Get(cacheKey).Result(); err == nil {
        var blob FileBlob
        json.Unmarshal([]byte(cached), &blob)
        return &blob, nil
    }

    // ç¼“å­˜æœªå‘½ä¸­ï¼Œä»æ•°æ®åº“æŸ¥è¯¢
    var blob FileBlob
    if err := db.Where("hash = ?", hash).First(&blob).Error; err != nil {
        return nil, err
    }

    // å†™å…¥ç¼“å­˜ (TTL 1 å°æ—¶)
    data, _ := json.Marshal(blob)
    redis.Set(cacheKey, data, 1*time.Hour)

    return &blob, nil
}
```

### 8.4 æ‰¹é‡æ“ä½œä¼˜åŒ–

```go
// æ‰¹é‡åˆ é™¤æ–‡ä»¶ï¼ˆé¿å… N+1 æŸ¥è¯¢ï¼‰
func batchDeleteFiles(fileIDs []string) error {
    tx := db.Begin()
    defer tx.Rollback()

    // æŸ¥è¯¢æ‰€æœ‰æ–‡ä»¶çš„å“ˆå¸Œå€¼
    var metadata []FilesMetadata
    if err := tx.Where("id IN ?", fileIDs).Find(&metadata).Error; err != nil {
        return err
    }

    // æ„å»ºå“ˆå¸Œåˆ—è¡¨
    hashes := make([]string, len(metadata))
    for i, m := range metadata {
        hashes[i] = m.FileBlobHash
    }

    // æ‰¹é‡è½¯åˆ é™¤
    if err := tx.Model(&FilesMetadata{}).
        Where("id IN ?", fileIDs).
        Update("deleted_at", time.Now()).Error; err != nil {
        return err
    }

    // æ‰¹é‡å‡å°‘å¼•ç”¨è®¡æ•°
    for _, hash := range hashes {
        tx.Exec("UPDATE file_blobs SET ref_count = ref_count - 1 WHERE hash = ?", hash)
    }

    tx.Commit()
    return nil
}
```

---

## ğŸ“š å‚è€ƒèµ„æ–™

- [Git: Content-Addressable Filesystem](https://git-scm.com/book/en/v2/Git-Internals-Git-Objects)
- [Restic: Design Principles](https://restic.readthedocs.io/en/latest/100_references.html#design)
- [Deduplication in Storage Systems](https://www.usenix.org/legacy/event/fast08/tech/full_papers/zhu/zhu.pdf)

---

**æ–‡æ¡£ç»´æŠ¤**: æœ¬æ–‡æ¡£åº”ä¸ `server/internal/storage` å®ç°ä¿æŒåŒæ­¥ã€‚

**æœ€åå®¡æ ¸**: 2026-02-04
